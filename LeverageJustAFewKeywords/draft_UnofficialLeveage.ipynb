{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "draft_UnofficialLeveage.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pFKd_3gBqo5"
      },
      "source": [
        "# LeverageJustAFewKeywords\n",
        "\n",
        "https://github.com/aqweteddy/LeverageJustAFewKeywords\n",
        "\n",
        "This notebook is for making things more clear, looking inside the data structure.\n",
        "\n",
        "**Experiments are run using Colab**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU2BJXR7BwtL",
        "outputId": "3ca1183e-a3d8-42a9-8084-6e26314ab812"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udW6lfpyB5W0",
        "outputId": "ed9aff10-b949-458b-cd0c-1ee4ac783f3d"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.6.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8fLK6xJCHGJ",
        "outputId": "1c94e02f-d15c-4f77-b733-3366769bced3"
      },
      "source": [
        "%cd /content/drive/MyDrive/LeverageJustAFewKeywords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/LeverageJustAFewKeywords\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ssz2Or1tBqo9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a60818-d06c-4624-9cb9-ef558b3b7150"
      },
      "source": [
        "%cd ../LeverageJustAFewKeywords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/LeverageJustAFewKeywords\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmRcsJBmBqo_"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbBHF5KBqo_"
      },
      "source": [
        "# `train.py`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edolELc4JcQ6"
      },
      "source": [
        "## Teacher Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rF0TK_aGJU3a"
      },
      "source": [
        "# from dataset import TestDataset\n",
        "from train import Trainer\n",
        "import torch\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai23clKmKdUe"
      },
      "source": [
        "def test(loader, model):\n",
        "    score = []\n",
        "    result = []\n",
        "    ground = []\n",
        "    for batch in loader:\n",
        "        idx, bow, labels = batch\n",
        "        bow = bow.cuda()\n",
        "        labels = labels.cuda()\n",
        "        model.z = model.reset_z()\n",
        "        with torch.no_grad():\n",
        "            logits = model.teacher(bow, model.z)\n",
        "        result.append(logits.max(-1)[1].cpu())\n",
        "        ground.append(labels.max(-1)[1].cpu())\n",
        "    result = torch.cat(result, dim=0)\n",
        "    ground = torch.cat(ground, dim=0)\n",
        "    score = f1_score(ground.numpy(), result.numpy(), average='micro')\n",
        "    # print(model.z)\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgEvigUWzIMD"
      },
      "source": [
        "general_aspect = {'bags_and_cases': 4, 'bluetooth': 6, 'boots': 5, 'keyboards': 7, 'tv': 5, 'vacuums': 5}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psEzVXG1KE8i"
      },
      "source": [
        "domain_range = ['bags_and_cases', 'bluetooth', 'boots', 'keyboards', 'tv', 'vacuums']\n",
        "aspect_name = domain_range[5]\n",
        "hparams = {\n",
        "    'lr': 5e-5,\n",
        "    'batch_size': 64,\n",
        "    'student': {\n",
        "        'pretrained': 'bert-base-uncased',\n",
        "        'pretrained_dim': 768,\n",
        "        'num_aspect': 9,\n",
        "    },\n",
        "    'description': f'{aspect_name} baseline',\n",
        "    'save_dir': f'./ckpt/{aspect_name}',\n",
        "    'train_file': f'./data/{aspect_name}_train.json',\n",
        "    'aspect_init_file': f'./data/{aspect_name}.30.txt',\n",
        "    'test_file': f'./data/{aspect_name}_test.json',\n",
        "    'general_asp': general_aspect[aspect_name],\n",
        "    'maxlen': 40\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFktkPwuKR-7",
        "outputId": "55e27ffc-d9ba-450e-ca85-9815c10c7409"
      },
      "source": [
        "trainer = Trainer(hparams, 'cuda')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:loading dataset...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "asp_category: 9\n",
            "bag_dim: 239\n",
            "loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:dataset_size: 1465525\n",
            "INFO:root:loading model...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "asp_category: 9\n",
            "bag_dim: 239\n",
            "length of data: 741; \tlength_of_labels: 741\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJUOvljxO_W9",
        "outputId": "3c274a40-b28d-4721-ee9e-e13c91db4e6d"
      },
      "source": [
        "test(trainer.test_loader, trainer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5614035087719298"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyEFHTszBqo_"
      },
      "source": [
        "check gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S78xo_hGBqpA"
      },
      "source": [
        "from train import *\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dbq4mksEBqpA"
      },
      "source": [
        "hparams = {\n",
        "    'lr': 5e-5,\n",
        "    'batch_size': 64,\n",
        "    'student': {\n",
        "        'pretrained': 'bert-base-uncased',\n",
        "        'pretrained_dim': 768,\n",
        "        'num_aspect': 9,\n",
        "    },\n",
        "    'description': 'bags_and_cases baseline',\n",
        "    'save_dir': './ckpt/bags',\n",
        "    'aspect_init_file': './data/bags_and_cases.30.txt',\n",
        "    'train_file': './data/bags_and_cases_train.json',\n",
        "    'test_file': './data/bags_and_cases_test.json',\n",
        "    'general_asp': 4,\n",
        "    'maxlen': 40\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohgEgB9mBqpA",
        "outputId": "4da177df-b5a8-4d37-bca8-23ea45360f9e"
      },
      "source": [
        "trainer = Trainer(hparams, 'cuda')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:loading dataset...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "asp_category: 9\n",
            "bag_dim: 226\n",
            "loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:root:dataset_size: 588229\n",
            "INFO:root:loading model...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "asp_category: 9\n",
            "bag_dim: 226\n",
            "653 653\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTG_e8CLBqpB"
      },
      "source": [
        "loader = DataLoader(trainer.ds, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-APRJawWBqpB"
      },
      "source": [
        "bow, id = next(iter(loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkEmfc4nBqpB",
        "outputId": "19b42a52-3d9f-4bb9-a81c-d0585a05ee0c"
      },
      "source": [
        "step_loss = trainer.train_step(bow.cuda(), id.cuda())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8r140X1EK0G"
      },
      "source": [
        "There is no trainable parameters in teacher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pE2-LOyCbw8",
        "outputId": "a3a066c6-2271-4ff3-b47f-552010a2d62c"
      },
      "source": [
        "for name, weight in trainer.student.named_parameters():\n",
        "    if weight.requires_grad:\n",
        "        if weight.grad == None:\n",
        "            print(name, 'grad none')\n",
        "        else:\n",
        "            print(name, weight.grad.mean())\n",
        "    else:\n",
        "        print(\"don't requires grad: {}\".format(name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert.embeddings.word_embeddings.weight tensor(-3.2293e-13, device='cuda:0')\n",
            "bert.embeddings.position_embeddings.weight tensor(-3.1984e-11, device='cuda:0')\n",
            "bert.embeddings.token_type_embeddings.weight tensor(-4.9671e-09, device='cuda:0')\n",
            "bert.embeddings.LayerNorm.weight tensor(0.0125, device='cuda:0')\n",
            "bert.embeddings.LayerNorm.bias tensor(0.0011, device='cuda:0')\n",
            "bert.encoder.layer.0.attention.self.query.weight tensor(-6.6983e-05, device='cuda:0')\n",
            "bert.encoder.layer.0.attention.self.query.bias tensor(0.0021, device='cuda:0')\n",
            "bert.encoder.layer.0.attention.self.key.weight tensor(8.9338e-05, device='cuda:0')\n",
            "bert.encoder.layer.0.attention.self.key.bias tensor(2.7759e-10, device='cuda:0')\n",
            "bert.encoder.layer.0.attention.self.value.weight tensor(-7.1446e-05, device='cuda:0')\n",
            "bert.encoder.layer.0.attention.self.value.bias tensor(-0.0016, device='cuda:0')\n",
            "bert.encoder.layer.0.attention.output.dense.weight tensor(0., device='cuda:0')\n",
            "bert.encoder.layer.0.attention.output.dense.bias tensor(-8.9252e-10, device='cuda:0')\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight tensor(0.0063, device='cuda:0')\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias tensor(0.0016, device='cuda:0')\n",
            "bert.encoder.layer.0.intermediate.dense.weight tensor(-4.6811e-05, device='cuda:0')\n",
            "bert.encoder.layer.0.intermediate.dense.bias tensor(0.0006, device='cuda:0')\n",
            "bert.encoder.layer.0.output.dense.weight tensor(-1.2935e-11, device='cuda:0')\n",
            "bert.encoder.layer.0.output.dense.bias tensor(-3.2984e-10, device='cuda:0')\n",
            "bert.encoder.layer.0.output.LayerNorm.weight tensor(-0.0027, device='cuda:0')\n",
            "bert.encoder.layer.0.output.LayerNorm.bias tensor(0.0623, device='cuda:0')\n",
            "bert.encoder.layer.1.attention.self.query.weight tensor(2.0860e-05, device='cuda:0')\n",
            "bert.encoder.layer.1.attention.self.query.bias tensor(-0.0012, device='cuda:0')\n",
            "bert.encoder.layer.1.attention.self.key.weight tensor(-1.4270e-06, device='cuda:0')\n",
            "bert.encoder.layer.1.attention.self.key.bias tensor(-6.2132e-11, device='cuda:0')\n",
            "bert.encoder.layer.1.attention.self.value.weight tensor(7.6686e-05, device='cuda:0')\n",
            "bert.encoder.layer.1.attention.self.value.bias tensor(-0.0040, device='cuda:0')\n",
            "bert.encoder.layer.1.attention.output.dense.weight tensor(9.7013e-12, device='cuda:0')\n",
            "bert.encoder.layer.1.attention.output.dense.bias tensor(-1.5522e-10, device='cuda:0')\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight tensor(0.0045, device='cuda:0')\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias tensor(0.0016, device='cuda:0')\n",
            "bert.encoder.layer.1.intermediate.dense.weight tensor(-4.0927e-05, device='cuda:0')\n",
            "bert.encoder.layer.1.intermediate.dense.bias tensor(0.0005, device='cuda:0')\n",
            "bert.encoder.layer.1.output.dense.weight tensor(1.2935e-11, device='cuda:0')\n",
            "bert.encoder.layer.1.output.dense.bias tensor(-3.1044e-10, device='cuda:0')\n",
            "bert.encoder.layer.1.output.LayerNorm.weight tensor(-0.0050, device='cuda:0')\n",
            "bert.encoder.layer.1.output.LayerNorm.bias tensor(0.0048, device='cuda:0')\n",
            "bert.encoder.layer.2.attention.self.query.weight tensor(9.2961e-06, device='cuda:0')\n",
            "bert.encoder.layer.2.attention.self.query.bias tensor(-0.0005, device='cuda:0')\n",
            "bert.encoder.layer.2.attention.self.key.weight tensor(-1.3242e-06, device='cuda:0')\n",
            "bert.encoder.layer.2.attention.self.key.bias tensor(2.5989e-11, device='cuda:0')\n",
            "bert.encoder.layer.2.attention.self.value.weight tensor(7.2125e-05, device='cuda:0')\n",
            "bert.encoder.layer.2.attention.self.value.bias tensor(-0.0034, device='cuda:0')\n",
            "bert.encoder.layer.2.attention.output.dense.weight tensor(2.1019e-11, device='cuda:0')\n",
            "bert.encoder.layer.2.attention.output.dense.bias tensor(-4.6566e-10, device='cuda:0')\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight tensor(0.0020, device='cuda:0')\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias tensor(-0.0011, device='cuda:0')\n",
            "bert.encoder.layer.2.intermediate.dense.weight tensor(-4.2461e-06, device='cuda:0')\n",
            "bert.encoder.layer.2.intermediate.dense.bias tensor(0.0002, device='cuda:0')\n",
            "bert.encoder.layer.2.output.dense.weight tensor(-6.4675e-12, device='cuda:0')\n",
            "bert.encoder.layer.2.output.dense.bias tensor(-7.7610e-11, device='cuda:0')\n",
            "bert.encoder.layer.2.output.LayerNorm.weight tensor(0.0010, device='cuda:0')\n",
            "bert.encoder.layer.2.output.LayerNorm.bias tensor(0.0053, device='cuda:0')\n",
            "bert.encoder.layer.3.attention.self.query.weight tensor(7.0112e-07, device='cuda:0')\n",
            "bert.encoder.layer.3.attention.self.query.bias tensor(9.6027e-06, device='cuda:0')\n",
            "bert.encoder.layer.3.attention.self.key.weight tensor(1.8251e-06, device='cuda:0')\n",
            "bert.encoder.layer.3.attention.self.key.bias tensor(7.2115e-11, device='cuda:0')\n",
            "bert.encoder.layer.3.attention.self.value.weight tensor(4.3133e-05, device='cuda:0')\n",
            "bert.encoder.layer.3.attention.self.value.bias tensor(-0.0019, device='cuda:0')\n",
            "bert.encoder.layer.3.attention.output.dense.weight tensor(-9.0949e-12, device='cuda:0')\n",
            "bert.encoder.layer.3.attention.output.dense.bias tensor(-3.8805e-10, device='cuda:0')\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight tensor(-0.0002, device='cuda:0')\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias tensor(8.6009e-05, device='cuda:0')\n",
            "bert.encoder.layer.3.intermediate.dense.weight tensor(4.4682e-06, device='cuda:0')\n",
            "bert.encoder.layer.3.intermediate.dense.bias tensor(-0.0001, device='cuda:0')\n",
            "bert.encoder.layer.3.output.dense.weight tensor(1.2935e-11, device='cuda:0')\n",
            "bert.encoder.layer.3.output.dense.bias tensor(-3.1044e-10, device='cuda:0')\n",
            "bert.encoder.layer.3.output.LayerNorm.weight tensor(-0.0010, device='cuda:0')\n",
            "bert.encoder.layer.3.output.LayerNorm.bias tensor(0.0060, device='cuda:0')\n",
            "bert.encoder.layer.4.attention.self.query.weight tensor(-9.0793e-06, device='cuda:0')\n",
            "bert.encoder.layer.4.attention.self.query.bias tensor(0.0003, device='cuda:0')\n",
            "bert.encoder.layer.4.attention.self.key.weight tensor(1.6197e-07, device='cuda:0')\n",
            "bert.encoder.layer.4.attention.self.key.bias tensor(4.7123e-11, device='cuda:0')\n",
            "bert.encoder.layer.4.attention.self.value.weight tensor(2.4636e-05, device='cuda:0')\n",
            "bert.encoder.layer.4.attention.self.value.bias tensor(-0.0013, device='cuda:0')\n",
            "bert.encoder.layer.4.attention.output.dense.weight tensor(-1.0105e-11, device='cuda:0')\n",
            "bert.encoder.layer.4.attention.output.dense.bias tensor(7.7610e-11, device='cuda:0')\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight tensor(0.0002, device='cuda:0')\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias tensor(-0.0007, device='cuda:0')\n",
            "bert.encoder.layer.4.intermediate.dense.weight tensor(2.0891e-06, device='cuda:0')\n",
            "bert.encoder.layer.4.intermediate.dense.bias tensor(-0.0001, device='cuda:0')\n",
            "bert.encoder.layer.4.output.dense.weight tensor(1.6169e-11, device='cuda:0')\n",
            "bert.encoder.layer.4.output.dense.bias tensor(-3.4925e-10, device='cuda:0')\n",
            "bert.encoder.layer.4.output.LayerNorm.weight tensor(-0.0009, device='cuda:0')\n",
            "bert.encoder.layer.4.output.LayerNorm.bias tensor(-0.0069, device='cuda:0')\n",
            "bert.encoder.layer.5.attention.self.query.weight tensor(-5.4219e-06, device='cuda:0')\n",
            "bert.encoder.layer.5.attention.self.query.bias tensor(0.0002, device='cuda:0')\n",
            "bert.encoder.layer.5.attention.self.key.weight tensor(-1.2012e-06, device='cuda:0')\n",
            "bert.encoder.layer.5.attention.self.key.bias tensor(-1.3567e-11, device='cuda:0')\n",
            "bert.encoder.layer.5.attention.self.value.weight tensor(8.7846e-06, device='cuda:0')\n",
            "bert.encoder.layer.5.attention.self.value.bias tensor(-0.0010, device='cuda:0')\n",
            "bert.encoder.layer.5.attention.output.dense.weight tensor(2.8295e-12, device='cuda:0')\n",
            "bert.encoder.layer.5.attention.output.dense.bias tensor(-2.1343e-10, device='cuda:0')\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight tensor(-0.0021, device='cuda:0')\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias tensor(0.0020, device='cuda:0')\n",
            "bert.encoder.layer.5.intermediate.dense.weight tensor(9.8142e-06, device='cuda:0')\n",
            "bert.encoder.layer.5.intermediate.dense.bias tensor(-0.0004, device='cuda:0')\n",
            "bert.encoder.layer.5.output.dense.weight tensor(-3.2338e-12, device='cuda:0')\n",
            "bert.encoder.layer.5.output.dense.bias tensor(1.5522e-10, device='cuda:0')\n",
            "bert.encoder.layer.5.output.LayerNorm.weight tensor(-0.0009, device='cuda:0')\n",
            "bert.encoder.layer.5.output.LayerNorm.bias tensor(0.0046, device='cuda:0')\n",
            "bert.encoder.layer.6.attention.self.query.weight tensor(-1.1682e-06, device='cuda:0')\n",
            "bert.encoder.layer.6.attention.self.query.bias tensor(4.5819e-05, device='cuda:0')\n",
            "bert.encoder.layer.6.attention.self.key.weight tensor(2.6856e-06, device='cuda:0')\n",
            "bert.encoder.layer.6.attention.self.key.bias tensor(-1.3832e-12, device='cuda:0')\n",
            "bert.encoder.layer.6.attention.self.value.weight tensor(5.7647e-06, device='cuda:0')\n",
            "bert.encoder.layer.6.attention.self.value.bias tensor(-8.6830e-05, device='cuda:0')\n",
            "bert.encoder.layer.6.attention.output.dense.weight tensor(4.2443e-12, device='cuda:0')\n",
            "bert.encoder.layer.6.attention.output.dense.bias tensor(-1.5522e-10, device='cuda:0')\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight tensor(0.0001, device='cuda:0')\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias tensor(0.0024, device='cuda:0')\n",
            "bert.encoder.layer.6.intermediate.dense.weight tensor(6.1706e-06, device='cuda:0')\n",
            "bert.encoder.layer.6.intermediate.dense.bias tensor(-0.0002, device='cuda:0')\n",
            "bert.encoder.layer.6.output.dense.weight tensor(1.6169e-11, device='cuda:0')\n",
            "bert.encoder.layer.6.output.dense.bias tensor(-2.3283e-10, device='cuda:0')\n",
            "bert.encoder.layer.6.output.LayerNorm.weight tensor(-0.0007, device='cuda:0')\n",
            "bert.encoder.layer.6.output.LayerNorm.bias tensor(0.0048, device='cuda:0')\n",
            "bert.encoder.layer.7.attention.self.query.weight tensor(3.3950e-06, device='cuda:0')\n",
            "bert.encoder.layer.7.attention.self.query.bias tensor(-0.0001, device='cuda:0')\n",
            "bert.encoder.layer.7.attention.self.key.weight tensor(-6.8496e-07, device='cuda:0')\n",
            "bert.encoder.layer.7.attention.self.key.bias tensor(-1.0681e-10, device='cuda:0')\n",
            "bert.encoder.layer.7.attention.self.value.weight tensor(2.9345e-05, device='cuda:0')\n",
            "bert.encoder.layer.7.attention.self.value.bias tensor(-0.0014, device='cuda:0')\n",
            "bert.encoder.layer.7.attention.output.dense.weight tensor(-8.0844e-13, device='cuda:0')\n",
            "bert.encoder.layer.7.attention.output.dense.bias tensor(-8.1491e-10, device='cuda:0')\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight tensor(0.0005, device='cuda:0')\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias tensor(0.0009, device='cuda:0')\n",
            "bert.encoder.layer.7.intermediate.dense.weight tensor(-1.2276e-07, device='cuda:0')\n",
            "bert.encoder.layer.7.intermediate.dense.bias tensor(-3.5969e-05, device='cuda:0')\n",
            "bert.encoder.layer.7.output.dense.weight tensor(2.7487e-11, device='cuda:0')\n",
            "bert.encoder.layer.7.output.dense.bias tensor(-6.9849e-10, device='cuda:0')\n",
            "bert.encoder.layer.7.output.LayerNorm.weight tensor(0.0006, device='cuda:0')\n",
            "bert.encoder.layer.7.output.LayerNorm.bias tensor(0.0126, device='cuda:0')\n",
            "bert.encoder.layer.8.attention.self.query.weight tensor(-9.9351e-06, device='cuda:0')\n",
            "bert.encoder.layer.8.attention.self.query.bias tensor(0.0003, device='cuda:0')\n",
            "bert.encoder.layer.8.attention.self.key.weight tensor(-2.8063e-06, device='cuda:0')\n",
            "bert.encoder.layer.8.attention.self.key.bias tensor(3.4314e-11, device='cuda:0')\n",
            "bert.encoder.layer.8.attention.self.value.weight tensor(2.6785e-05, device='cuda:0')\n",
            "bert.encoder.layer.8.attention.self.value.bias tensor(-0.0015, device='cuda:0')\n",
            "bert.encoder.layer.8.attention.output.dense.weight tensor(7.8823e-12, device='cuda:0')\n",
            "bert.encoder.layer.8.attention.output.dense.bias tensor(-5.4327e-10, device='cuda:0')\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight tensor(0.0002, device='cuda:0')\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias tensor(-9.5745e-05, device='cuda:0')\n",
            "bert.encoder.layer.8.intermediate.dense.weight tensor(1.0298e-06, device='cuda:0')\n",
            "bert.encoder.layer.8.intermediate.dense.bias tensor(2.0184e-05, device='cuda:0')\n",
            "bert.encoder.layer.8.output.dense.weight tensor(1.1318e-11, device='cuda:0')\n",
            "bert.encoder.layer.8.output.dense.bias tensor(-7.7610e-11, device='cuda:0')\n",
            "bert.encoder.layer.8.output.LayerNorm.weight tensor(-0.0026, device='cuda:0')\n",
            "bert.encoder.layer.8.output.LayerNorm.bias tensor(0.0032, device='cuda:0')\n",
            "bert.encoder.layer.9.attention.self.query.weight tensor(3.1629e-06, device='cuda:0')\n",
            "bert.encoder.layer.9.attention.self.query.bias tensor(-3.4532e-05, device='cuda:0')\n",
            "bert.encoder.layer.9.attention.self.key.weight tensor(1.2839e-06, device='cuda:0')\n",
            "bert.encoder.layer.9.attention.self.key.bias tensor(9.3820e-11, device='cuda:0')\n",
            "bert.encoder.layer.9.attention.self.value.weight tensor(-2.4716e-05, device='cuda:0')\n",
            "bert.encoder.layer.9.attention.self.value.bias tensor(0.0009, device='cuda:0')\n",
            "bert.encoder.layer.9.attention.output.dense.weight tensor(-1.3743e-11, device='cuda:0')\n",
            "bert.encoder.layer.9.attention.output.dense.bias tensor(-9.3132e-10, device='cuda:0')\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight tensor(0.0016, device='cuda:0')\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias tensor(0.0011, device='cuda:0')\n",
            "bert.encoder.layer.9.intermediate.dense.weight tensor(-2.9188e-06, device='cuda:0')\n",
            "bert.encoder.layer.9.intermediate.dense.bias tensor(4.1211e-05, device='cuda:0')\n",
            "bert.encoder.layer.9.output.dense.weight tensor(-3.2338e-12, device='cuda:0')\n",
            "bert.encoder.layer.9.output.dense.bias tensor(-1.5522e-10, device='cuda:0')\n",
            "bert.encoder.layer.9.output.LayerNorm.weight tensor(-0.0033, device='cuda:0')\n",
            "bert.encoder.layer.9.output.LayerNorm.bias tensor(0.0006, device='cuda:0')\n",
            "bert.encoder.layer.10.attention.self.query.weight tensor(2.3009e-06, device='cuda:0')\n",
            "bert.encoder.layer.10.attention.self.query.bias tensor(-8.9178e-05, device='cuda:0')\n",
            "bert.encoder.layer.10.attention.self.key.weight tensor(-1.9040e-06, device='cuda:0')\n",
            "bert.encoder.layer.10.attention.self.key.bias tensor(2.9615e-11, device='cuda:0')\n",
            "bert.encoder.layer.10.attention.self.value.weight tensor(-5.7444e-06, device='cuda:0')\n",
            "bert.encoder.layer.10.attention.self.value.bias tensor(0.0008, device='cuda:0')\n",
            "bert.encoder.layer.10.attention.output.dense.weight tensor(1.0914e-11, device='cuda:0')\n",
            "bert.encoder.layer.10.attention.output.dense.bias tensor(-3.1044e-10, device='cuda:0')\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight tensor(0.0017, device='cuda:0')\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias tensor(-0.0005, device='cuda:0')\n",
            "bert.encoder.layer.10.intermediate.dense.weight tensor(-2.3448e-06, device='cuda:0')\n",
            "bert.encoder.layer.10.intermediate.dense.bias tensor(5.0552e-05, device='cuda:0')\n",
            "bert.encoder.layer.10.output.dense.weight tensor(-1.2935e-11, device='cuda:0')\n",
            "bert.encoder.layer.10.output.dense.bias tensor(0., device='cuda:0')\n",
            "bert.encoder.layer.10.output.LayerNorm.weight tensor(0.0051, device='cuda:0')\n",
            "bert.encoder.layer.10.output.LayerNorm.bias tensor(-0.0072, device='cuda:0')\n",
            "bert.encoder.layer.11.attention.self.query.weight tensor(-2.7417e-06, device='cuda:0')\n",
            "bert.encoder.layer.11.attention.self.query.bias tensor(8.6908e-05, device='cuda:0')\n",
            "bert.encoder.layer.11.attention.self.key.weight tensor(-2.3581e-06, device='cuda:0')\n",
            "bert.encoder.layer.11.attention.self.key.bias tensor(2.3078e-11, device='cuda:0')\n",
            "bert.encoder.layer.11.attention.self.value.weight tensor(4.7503e-06, device='cuda:0')\n",
            "bert.encoder.layer.11.attention.self.value.bias tensor(-5.4396e-06, device='cuda:0')\n",
            "bert.encoder.layer.11.attention.output.dense.weight tensor(6.4675e-12, device='cuda:0')\n",
            "bert.encoder.layer.11.attention.output.dense.bias tensor(1.5522e-10, device='cuda:0')\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight tensor(-0.0006, device='cuda:0')\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias tensor(-0.0013, device='cuda:0')\n",
            "bert.encoder.layer.11.intermediate.dense.weight tensor(-8.4920e-06, device='cuda:0')\n",
            "bert.encoder.layer.11.intermediate.dense.bias tensor(0.0002, device='cuda:0')\n",
            "bert.encoder.layer.11.output.dense.weight tensor(-1.6169e-12, device='cuda:0')\n",
            "bert.encoder.layer.11.output.dense.bias tensor(0., device='cuda:0')\n",
            "bert.encoder.layer.11.output.LayerNorm.weight tensor(-0.0056, device='cuda:0')\n",
            "bert.encoder.layer.11.output.LayerNorm.bias tensor(0.0014, device='cuda:0')\n",
            "bert.pooler.dense.weight grad none\n",
            "bert.pooler.dense.bias grad none\n",
            "fc.0.weight tensor(-1.2202e-09, device='cuda:0')\n",
            "fc.0.bias tensor(9.9341e-08, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOVVryNTBqpC",
        "outputId": "510bd556-ed57-4dd8-db52-c9d1f30df40c"
      },
      "source": [
        "0 / 1e-10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-wtnw2CBqpC"
      },
      "source": [
        "# `model.py`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhpU54GJBqpD"
      },
      "source": [
        "## Teacher\n",
        "\n",
        "Would it be a problem to just assign 1 as appearance times? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S105whtCBqpD",
        "outputId": "50ca5d78-4b63-4fa8-c5d1-f63467485510"
      },
      "source": [
        "from scipy.special import softmax\n",
        "import numpy as np\n",
        "x = np.zeros(40)\n",
        "x[1] = \n",
        "m = softmax(x)\n",
        "print(m)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0. nan  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/special/_logsumexp.py:215: RuntimeWarning: invalid value encountered in subtract\n",
            "  return np.exp(x - logsumexp(x, axis=axis, keepdims=True))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PINYdyr8HtDX",
        "outputId": "857c64a7-5761-4ae2-a477-8440e10fd70c"
      },
      "source": [
        "# a = torch.rand(3,4)\n",
        "a = torch.arange(12).view(3,4)\n",
        "# b = torch.rand(4)\n",
        "b = torch.arange(4)\n",
        "print(a, b)\n",
        "c = b * a\n",
        "print(c)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]]) tensor([0, 1, 2, 3])\n",
            "tensor([[ 0,  1,  4,  9],\n",
            "        [ 0,  5, 12, 21],\n",
            "        [ 0,  9, 20, 33]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFLr2YpiBqpD"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5v9nCHgBqpD"
      },
      "source": [
        "mat = torch.tensor([[0.6, 0.0, 1, 0.0],\n",
        "                    [0.0, 0.4, 0.0, 0.0],\n",
        "                    [0.0, 0.0, 1.2, 0.0],\n",
        "                    [0.0, 0.0, 0.0,-0.4]])\n",
        "idx = torch.nonzero(mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C81XBaTvBqpE"
      },
      "source": [
        "mat[torch.where(mat==0)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv5Y0ExMBqpE"
      },
      "source": [
        "mat.max(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJABMAYTBqpE"
      },
      "source": [
        "torch.where(mat.max(1)[1] == 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnv-ub9GBqpE"
      },
      "source": [
        "# `dataset.py`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o52ihtO3BqpE"
      },
      "source": [
        "from dataset import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kqe50Y9qBqpF"
      },
      "source": [
        "## `TestDataset`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSjmom78BqpF"
      },
      "source": [
        "aspect_init_file = \"./data/bags_and_cases.30.txt\"\n",
        "data_file = \"./data/bags_and_cases_test.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CuLau2UBqpF",
        "outputId": "6ba8b7e8-7e7e-4edd-9bc7-582006b9148c"
      },
      "source": [
        "test_ds = TestDataset(aspect_init_file, data_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading dataset...\n",
            "asp_category: 9\n",
            "bag_dim: 226\n",
            "length of data: 653; \tlength_of_labels: 653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2S-LX95BqpG"
      },
      "source": [
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sr8VgKQxBqpG"
      },
      "source": [
        "test_loader = DataLoader(test_ds, batch_size=500, num_workers=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VP1xyOddBqpG"
      },
      "source": [
        "idx, bow, labels = next(iter(test_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9AJYQtxBqpG"
      },
      "source": [
        "print(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdsGb9U0BqpH"
      },
      "source": [
        "## `Dataset`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M5Va2R6BqpH"
      },
      "source": [
        "aspect_init_file = \"./data/boots.5.txt\"\n",
        "data_file = \"./data/boots_train.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsFoUuEdBqpH",
        "outputId": "b21e26d3-3797-46f2-9620-f2131de8f334"
      },
      "source": [
        "ds = Dataset(aspect_init_file, data_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 541kB/s]\n",
            "Downloading: 100%|██████████| 466k/466k [00:00<00:00, 478kB/s]\n",
            "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 28.0kB/s]\n",
            "asp_category: 9\n",
            "bag_dim: 44\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GThp_jJJBqpH",
        "outputId": "50827bca-9589-491a-9f8b-c46752488613"
      },
      "source": [
        "print(ds.aspects, '\\n')\n",
        "print(ds.id2asp, '\\n')\n",
        "print(ds.asp2id, '\\n')\n",
        "print(ds.aspect_ids, '\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['color', 'love', 'style', 'unbelievably', 'gorgeous'], ['comfortable', 'fit', 'foot', 'hurt', 'ankle'], ['rubber', 'quality', 'use', 'buckle', 'appear'], ['cute', 'look', 'looked', 'great', 'fringe'], ['leather', 'inside', 'fringe', 'material', 'heel'], ['return', 'pair', 'thought', 'year', 'boot'], ['worth', 'money', 'cheap', 'inexpensive', 'paid'], ['size', 'ordered', 'half', 'order', 'big'], ['dry', 'waterproof', 'rain', 'wet', 'water']] \n",
            "\n",
            "{0: 'ankle', 1: 'appear', 2: 'big', 3: 'boot', 4: 'buckle', 5: 'cheap', 6: 'color', 7: 'comfortable', 8: 'cute', 9: 'dry', 10: 'fit', 11: 'foot', 12: 'fringe', 13: 'gorgeous', 14: 'great', 15: 'half', 16: 'heel', 17: 'hurt', 18: 'inexpensive', 19: 'inside', 20: 'leather', 21: 'look', 22: 'looked', 23: 'love', 24: 'material', 25: 'money', 26: 'order', 27: 'ordered', 28: 'paid', 29: 'pair', 30: 'quality', 31: 'rain', 32: 'return', 33: 'rubber', 34: 'size', 35: 'style', 36: 'thought', 37: 'unbelievably', 38: 'use', 39: 'water', 40: 'waterproof', 41: 'wet', 42: 'worth', 43: 'year'} \n",
            "\n",
            "{'ankle': 0, 'appear': 1, 'big': 2, 'boot': 3, 'buckle': 4, 'cheap': 5, 'color': 6, 'comfortable': 7, 'cute': 8, 'dry': 9, 'fit': 10, 'foot': 11, 'fringe': 12, 'gorgeous': 13, 'great': 14, 'half': 15, 'heel': 16, 'hurt': 17, 'inexpensive': 18, 'inside': 19, 'leather': 20, 'look': 21, 'looked': 22, 'love': 23, 'material': 24, 'money': 25, 'order': 26, 'ordered': 27, 'paid': 28, 'pair': 29, 'quality': 30, 'rain': 31, 'return': 32, 'rubber': 33, 'size': 34, 'style': 35, 'thought': 36, 'unbelievably': 37, 'use': 38, 'water': 39, 'waterproof': 40, 'wet': 41, 'worth': 42, 'year': 43} \n",
            "\n",
            "[[6, 23, 35, 37, 13], [7, 10, 11, 17, 0], [33, 30, 38, 4, 1], [8, 21, 22, 14, 12], [20, 19, 12, 24, 16], [32, 29, 36, 43, 3], [42, 25, 5, 18, 28], [34, 27, 15, 26, 2], [9, 40, 31, 41, 39]] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjHYxuKzBqpI",
        "outputId": "9ce2ff76-d630-44f9-bf6f-4941436cc2a4"
      },
      "source": [
        "word_dict = {}\n",
        "with open(aspect_init_file, 'r') as f:\n",
        "    for line in f.readlines():\n",
        "        l = line.strip().split(' ')\n",
        "        words = l[0:]\n",
        "        for w in words:\n",
        "            if w in word_dict.keys():\n",
        "                word_dict[w] += 1\n",
        "            else:\n",
        "                word_dict[w] = 1\n",
        "print(word_dict)\n",
        "# or use collections.Counter()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'color': 1, 'love': 1, 'style': 1, 'unbelievably': 1, 'gorgeous': 1, 'comfortable': 1, 'fit': 1, 'foot': 1, 'hurt': 1, 'ankle': 1, 'rubber': 1, 'quality': 1, 'use': 1, 'buckle': 1, 'appear': 1, 'cute': 1, 'look': 1, 'looked': 1, 'great': 1, 'fringe': 2, 'leather': 1, 'inside': 1, 'material': 1, 'heel': 1, 'return': 1, 'pair': 1, 'thought': 1, 'year': 1, 'boot': 1, 'worth': 1, 'money': 1, 'cheap': 1, 'inexpensive': 1, 'paid': 1, 'size': 1, 'ordered': 1, 'half': 1, 'order': 1, 'big': 1, 'dry': 1, 'waterproof': 1, 'rain': 1, 'wet': 1, 'water': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xuV7aTCtBqpI",
        "outputId": "1b90c039-00f4-4020-8d0d-780af529b3d8"
      },
      "source": [
        "idx2asp = ds.get_idx2asp()\n",
        "n_word_per_aspect = len(ds.aspects[0])\n",
        "print(idx2asp)\n",
        "print(len(idx2asp))\n",
        "print(idx2asp[ds.asp2id['fringe']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 7, 5, 2, 6, 0, 1, 3, 8, 1, 1, 3, 0, 3, 7, 4, 1, 6, 4, 4, 3, 3, 0, 4, 6, 7, 7, 6, 5, 2, 8, 5, 2, 7, 0, 5, 0, 2, 8, 8, 8, 6, 5]\n",
            "44\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kxv5bUGBqpI"
      },
      "source": [
        "**Note**: word fringe appears twice, and it was assigned to the category in which it first appear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDQ_eQkqBqpJ",
        "outputId": "6945a42a-e8cf-4351-d455-f40a0defafa0"
      },
      "source": [
        "print(len(ds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "963866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TATkMXWBqpJ"
      },
      "source": [
        "`__getitem__` method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvwPoPkJBqpJ",
        "outputId": "c2c3b207-48e2-481a-fd3b-8adab428dc95"
      },
      "source": [
        "eg = ds.data[740000]\n",
        "print(eg)\n",
        "bows = ds.vectorizer.transform([eg]).toarray()\n",
        "print(bows)\n",
        "print(np.where(bows==1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I disagree with other reviewers in terms of size .\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 0 0 0 0 0]]\n",
            "(array([0], dtype=int64), array([34], dtype=int64))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IedeZQpfBqpK"
      },
      "source": [
        "# `extract_data.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6E5OURzBqpK"
      },
      "source": [
        "import h5py\n",
        "from tqdm import tqdm\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuM8SzFnBqpK"
      },
      "source": [
        "source = \"./data/preprocessed/BOOTS_MATE.hdf5\"\n",
        "# source = \"./data/preprocessed/BOOTS_MATE_TEST.hdf5\"\n",
        "output = \"./data/boots_train.json\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWCdoMHWBqpK"
      },
      "source": [
        "def load_h5(f, label):\n",
        "    size = len(f.get(label))\n",
        "    result = [f.get(label).get(str(i))[()].squeeze().tolist() for i in tqdm(range(size))]\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kejSV0cBqpL"
      },
      "source": [
        "f.close()\n",
        "f = h5py.File(source, 'r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nX7u0oqpBqpL",
        "outputId": "6c5ef178-4561-4520-8406-9f323cef874f"
      },
      "source": [
        "print(len(f.get('original')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19278\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoMjI_09BqpL",
        "outputId": "84655b3b-d933-41e6-b839-6a45ea6de950"
      },
      "source": [
        "# a = f.get('original').get('1')[()].squeeze().tolist()\n",
        "a = f.get('data').get(str('12'))[()].tolist()\n",
        "print(a)\n",
        "print(type(a))\n",
        "print(len(a))\n",
        "print(a[0])\n",
        "print(type(a[0]) == bytes)\n",
        "isinstance(a[0], bytes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[983, 96, 737, 99, 1364, 5071, 976, 1033, 973, 149, 12, 8810, 381, 3821]]\n",
            "<class 'list'>\n",
            "1\n",
            "[983, 96, 737, 99, 1364, 5071, 976, 1033, 973, 149, 12, 8810, 381, 3821]\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA-mzsOiBqpL",
        "outputId": "135af87f-7090-4774-fc64-67bda90be695"
      },
      "source": [
        "list(map(lambda x:x.decode(), f.get('original').get('0')[()].squeeze().tolist()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['that I love ,',\n",
              " 'but these are a disappointment .',\n",
              " 'I do not like them at all ,',\n",
              " 'returning them .',\n",
              " 'They are not comfortable .',\n",
              " 'they gave me .',\n",
              " 'how i will wrap these .',\n",
              " 'I requested .',\n",
              " 'They said',\n",
              " 'This is the day',\n",
              " 'that was the boots',\n",
              " \"do n't have !\",\n",
              " 'Thanks',\n",
              " 'I bought',\n",
              " 'wide',\n",
              " 'that ok am',\n",
              " 'enjoying them',\n",
              " 'and they were too small .',\n",
              " 'I thought',\n",
              " 'I like them .',\n",
              " 'but i guess',\n",
              " 'which is good !',\n",
              " 'But i love them',\n",
              " 'they were too big .',\n",
              " 'to try them out .',\n",
              " 'Do remember',\n",
              " 'after breaking them in , so',\n",
              " '1 .',\n",
              " '2 .',\n",
              " '3 .',\n",
              " '4 .',\n",
              " 'she wanted .',\n",
              " 'It took a while',\n",
              " 'to get them ;',\n",
              " 'I guess',\n",
              " 'to replace them .',\n",
              " 'I buy',\n",
              " 'and have to say',\n",
              " 'So , I bought',\n",
              " 'I know how',\n",
              " \"but did n't .\",\n",
              " 'it was the boots',\n",
              " 'I wore them',\n",
              " 'To be fair ,',\n",
              " 'what I need to do',\n",
              " 'Ridiculous !',\n",
              " 'and try them on .',\n",
              " 'so it swallowed me up :',\n",
              " 'and the boots',\n",
              " 'It was absurd -']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXx0a2yXBqpM",
        "outputId": "7b580de9-1c6e-4f3a-89f2-5f544b5c5d21"
      },
      "source": [
        "data = {'original': load_h5(f, 'original')}\n",
        "data['original'] = [[o] if isinstance(o, str) else o for o in data['original']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 19278/19278 [00:06<00:00, 2934.68it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "ABuLfxj_BqpM",
        "outputId": "11948a1f-35a0-4eb0-92a3-b3f60856ac43"
      },
      "source": [
        "print(type(data['original']))\n",
        "for i in range(10):\n",
        "    print(data['original'][i])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n",
            "[b'that I love ,', b'but these are a disappointment .', b'I do not like them at all ,', b'returning them .', b'They are not comfortable .', b'they gave me .', b'how i will wrap these .', b'I requested .', b'They said', b'This is the day', b'that was the boots', b\"do n't have !\", b'Thanks', b'I bought', b'wide', b'that ok am', b'enjoying them', b'and they were too small .', b'I thought', b'I like them .', b'but i guess', b'which is good !', b'But i love them', b'they were too big .', b'to try them out .', b'Do remember', b'after breaking them in , so', b'1 .', b'2 .', b'3 .', b'4 .', b'she wanted .', b'It took a while', b'to get them ;', b'I guess', b'to replace them .', b'I buy', b'and have to say', b'So , I bought', b'I know how', b\"but did n't .\", b'it was the boots', b'I wore them', b'To be fair ,', b'what I need to do', b'Ridiculous !', b'and try them on .', b'so it swallowed me up :', b'and the boots', b'It was absurd -']\n",
            "[b'The pair', b'No matter ,', b'And , they . are . hard .', b'I know', b'Tight .', b'to review these ,', b'or whatever .', b'Not so much .', b'Probably not .', b'and may then ,', b'Love', b'I expected and more .', b'They are comfy ,', b'when you walk .', b'The only complaint', b'the are giving ,', b'The only thing', b'that is a bit', b'bothersome is', b'turned in', b'when I walk .', b'have said ,', b'I like', b'and I am disappointed', b'they are too small ,', b'I love them !!', b'I guess', b'she is keeping them', b'but this was extreme -RRB- ,', b'-LRB- which was not surprising ,', b'but after a while It was fine .', b'I guess', b'which can be uncomfortable', b'I wear .', b'you understand !', b'Other than the initial', b'she loves them', b'before deciding on these .', b'I wore them', b'to get her .', b'I ended up', b'and she said', b'if you think', b'order up .', b'but its nothing', b'I will post again', b'They looked', b'wearing them ...', b'-LRB- in length -RRB- .', b'I did not reorder ...']\n",
            "[b'and I knew', b'that when I ordered', b'what I paid !', b'been looking all over', b'because they are so comfortable .', b'not a problem', b'and she gets', b'that I tried in', b'They are very comfortable !', b'and probably will ,', b'and they are very comfortable .', b'and I can say', b'like I was ,', b'all I can say', b'is get them !!!', b'that could be', b'I was not disappointed ,', b'It not only fits ,', b'I wanted .', b'When they arrived', b'The ones', b'Perfect .', b'I try ,', b'THANKS !', b'I was told &#34;', b'broken in !', b'from slipping down .', b'-LRB- the wide', b'and I love all of them .', b'They are comfortable', b'I read', b'Sizing -', b'The minute', b'they were delivered ,', b'These boots !', b'LOVE THEM !', b'and wore', b'them til', b\"'til now !!\", b'I worried', b'and decided', b'they were worth', b'a try .', b'are very comfortable', b'and I love them .', b'very &#34; bendable &#34;', b'and I have no complaints ...', b'a boot should with having', b'you get', b'like it should be too .']\n",
            "[b'They are so comfortable', b'and they are beautiful .', b'I am so glad', b'I read', b'-LRB- and had faith in -RRB-', b'Yes ,', b'COMFORT .', b'amazon -RRB- .', b'and I was in love !', b'If you are on the fence ,', b'hop on over', b'so they did fit', b\"'s\", b'because theirs has support under them .', b'This was disappointing .', b'-- this was the one', b'Very comfortable .', b'They were a disappoint .', b'but they were damaged', b'so I had to return them .', b'in being let down again ...', b'but these were cheap .', b'and it looked', b'because they are so cute', b'and they were expensive', b'I had to return these .', b'I had wanted', b'When they arrived ,', b'because I can just slip those on', b'i think', b'If they stretch', b'will be too big .', b'as I walk ,', b'to prevent', b'and am hoping', b'I wear', b'Just an FYI :', b'I love', b'how comfy they are', b'and tried', b'them on ended up', b'they are so comfortable : -RRB-', b'I am so happy', b'but so worth it .', b'It goes', b'LOVE !', b'when I wear these : -RRB-', b'My friend', b'so I can wear them more .', b'to slide in and out of them']\n",
            "[b'where i was bleeding', b'you notice', b'and just be patient ,', b'its worth it .', b'it took me a while', b'When I had the laces', b'they slid', b'I feel', b'they support them', b'You can dress them up or down .', b'Ahhhh !', b'and when I found them', b'and I love all of them .', b'They fit', b'as I expected', b'you need it .', b'and I imagine', b'I m a 9 .', b'So soft .', b'And 10mins off .', b'I have only worn these once ,', b'and these were so comfortable', b'so I ordered', b'I feel', b'and thinks', b'they are very stylish', b'They are comfy', b'The only problem is', b'I been wanting them for a while', b'and I think', b'Love these !', b'I am so happy', b'where I got them ,', b'and it has been a nightmare', b'and I know', b'but I heard', b'Love them !!!!', b'LOVE !', b'CHINA ???', b'where made ,', b'but provided', b'we should be ashamed', b'I like them ,', b'as soon', b'as I got them .', b'This is 2011 and these', b'I have the ones', b'and I loved them .', b'The slippers', b'that they were not the same quality .']\n",
            "[b'I have had these a few months', b'I recommend', b'and put them on ,', b'and then thought', b'to be comfortable .', b'I know', b'Darn .', b'she says', b'I returned them', b'-LRB- of which I was expecting -RRB-', b'out of 5 .', b'This is shoe', b'and does the job !', b'I wear', b'and I think', b'LOVE THEM .', b'and are now perfect .', b'as they stretch', b'I hope', b'they never do .', b'They looked', b'None of them', b'and just received them .', b'The only thing', b'I was not happy about', b'However , now', b'I have dogs', b'These are thick ,', b'and I believe', b'but these Minnetonkas have won me over .', b'I wish', b'they were precisely', b'as I had hoped', b'they would be .', b'I have no regrets', b'As a husband ,', b'she loves them .', b'before ordering', b'after squeezing into them', b'and wearing them a while .', b'These are marvelous !', b'and she loves them', b'I think ,', b'etc. .', b'which is nice ,', b'to compare them to ,', b'being said ,', b'I think', b'I had to return them', b'because they were just not wearable .']\n",
            "[b'They looked', b'but they were too loose .', b'I went for these', b'I like', b'the look', b'Go for it !', b'I seemed', b'Had to return .', b'I wore', b'to fix them', b'I ended up', b'pulling both of them off', b'I wear .', b'Q', b'for some time now .', b'and was SO disappointed .', b'what I tried', b'But they were too small ,', b'than what you really are .', b'when I got them', b'I expected .', b'-LRB- : These boots', b'I guess ,', b'Period .', b'You should be okay .', b'And the bad :', b'to get them on .', b'Just a bit .', b'and went ,', b'`` Oh no !', b'Then I tried the other ,', b'and I discovered', b'That helped .', b'to help you !', b'if you really ,', b'so I worry', b'-LRB- once you get them on -RRB- .', b'if you plan', b'on tucking them in', b'I wear', b'Just reordered', b'than you wear', b'and it will be perfect !', b'that said', b'and you should be just fine .', b'so i thought', b'and were so comfortable', b'but it was to large', b'I love them .', b'and they were very comfortable']\n",
            "[b'They are very stylish .', b'where I got them .', b'This does the trick .', b'I was concerned', b\"while I 've been\", b'wearing them .', b'They are just as described .', b'and felt', b'just to keep them on .', b'Love them .', b'These boots were', b'when ordering', b'and looks .', b'that you will have a problem', b'with sizing', b'When I wear these out I', b'not sure', b'they move with you -', b'no stiffness at all ;', b'before I put them on .', b'I put them on .', b'and had to be broken in .', b'Did I mention', b'we are from Texas ?', b'She wears them', b'but the size was off for me .', b'to return them .', b'I was wrong .', b'They showed up', b'I am very disappointed .', b'so I ordered', b'-LRB- 9 -RRB-', b'Wrong !', b'I ordered', b'it looks ,', b'to dance in .', b'I had was amazing .', b'after having them for a year .', b'After I sold them I', b'If you dont own these', b'not too bad ,', b'if you are worried ,', b'and have had these boots before .', b'Easy on/off .', b'NOT as insulating', b'as I had hoped', b\"I 've had\", b'-LRB- which my feet do', b'My only complaint is', b'Might be me']\n",
            "[b'being picky ,', b'I was disappointed', b'when I walk .', b'-LRB- not same brand -RRB- ,', b'there was no way', b'to fix it .', b'which does not please me .', b'I do have bunions', b'They handle', b'Just started', b'wearing them .', b'and dry', b'has been true for me .', b'because they are comfortable .', b'if need be .', b'but anticipate', b'I wear', b'They will have to be broken in .', b'to wear .', b'-LRB- in china -RRB-', b'I was very disappointing', b'The only thing', b'It was comfortable', b'I figured', b'when it was made .', b'as I expected .', b'as we are bridesmaids .', b'When they arrived', b'I just wish', b'When I received them', b'I was very annoyed .', b'NOT happy at all !', b'When I tried them on ,', b'I knew', b'to compare them .', b'I know', b\"so I 'm\", b'keeping both', b'choose !', b'I needed them', b'so I compromised', b'and skirts .', b'Returned .', b\"this is n't it ,\", b'No barrel', b'racing in these !', b'I returned these', b'I thought', b'but I was wrong .', b'because of size .']\n",
            "[b'for riding .', b'if I think', b'and had no problems .', b'I guess', b'than that they are great !', b'I can add', b'Which is just what I wanted .', b'but that is not to say', b'which surprised me .', b'so that was no problem for me .', b'The insert', b'acknowledges', b'-LRB- honestly ! -RRB-', b'which I guess', b'it seems', b'to slip .', b'So dark', b'They are growing on me .', b'I think', b'It said', b'I thought', b'which I was very surprised .', b'but they were too small', b'but was concerned', b'The boot', b'-LRB- red -RRB-', b'is not very shiny', b'but it is not suede', b'nor have unusualtexture .', b'-LRB- with a Luchese -RRB-', b'LOVE Them !', b'and I wondered', b'She loves them', b'Otherwise ,', b'and she will love them !', b'and I love', b'how they look .', b'on sizing', b'There is no way', b'and love them .', b'but I can tell', b'-LRB- black -RRB-', b'No complaints .', b'Enjoy them !', b'to be as comfortable', b'but I know', b'They are definitely', b'that I was afraid', b'it would be', b'that I ordered']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lremWjLZBqpM",
        "outputId": "081993fc-373c-4609-941c-7d9176f844bf"
      },
      "source": [
        "data['label'] = load_h5(f, 'labels')\n",
        "data['label'] = [[o] if isinstance(o[0], int) else o for o in data['label']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-9-571ffd89a0cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_h5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'labels'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-2-36dd367b864b>\u001b[0m in \u001b[0;36mload_h5\u001b[1;34m(f, label)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_h5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqlNRif4BqpN",
        "outputId": "c554e63a-5631-4fed-adbd-d2a5b50c35bd"
      },
      "source": [
        "with open(output, 'w') as f:\n",
        "    json.dump(data, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type bytes is not JSON serializable",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-14-6712ae71ebd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mE:\\Anaconda3\\envs\\nlp\\lib\\json\\__init__.py\u001b[0m in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;31m# a debuggability cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mE:\\Anaconda3\\envs\\nlp\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 431\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    432\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mE:\\Anaconda3\\envs\\nlp\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mE:\\Anaconda3\\envs\\nlp\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode_list\u001b[1;34m(lst, _current_indent_level)\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mE:\\Anaconda3\\envs\\nlp\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode_list\u001b[1;34m(lst, _current_indent_level)\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m                 \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mE:\\Anaconda3\\envs\\nlp\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    436\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Circular reference detected\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 438\u001b[1;33m             \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mE:\\Anaconda3\\envs\\nlp\\lib\\json\\encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m--> 179\u001b[1;33m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[0;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: Object of type bytes is not JSON serializable"
          ]
        }
      ]
    }
  ]
}