{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e5ad35c58ba9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "# use gensim==3.8, there is a problem when using intersect_word2vec_format in gensim==4.0 \n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- fine tuning should be on the whole corpus, i.e. using both articles and comments\n",
    "- should we remove stop words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"../processed/comments.pkl\"\n",
    "df = pd.read_pickle(data_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = df['comment'].astype('str').tolist()\n",
    "print(len(sentences))\n",
    "print(sentences[0])\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "sentences_tokenized = [s.lower() for s in sentences]\n",
    "sentences_tokenized = [tokenizer.tokenize(i) for i in sentences_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(sentences_tokenized[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# processing with stop words and lemmatizing\n",
    "lemmatize = True\n",
    "remove_stop_word = False\n",
    "if lemmatize:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "if remove_stop_word:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "else:\n",
    "    stop_words = None\n",
    "sentences_step2 = []\n",
    "for tokens in sentences_tokenized:\n",
    "    if stop_words:\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "    if lemmatize:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    sentences_step2.append(tokens)\n",
    "print(sentences_step2[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(size=300, min_count=5)\n",
    "# model.build_vocab(sentences_tokenized)\n",
    "model.build_vocab(sentences_step2)\n",
    "total_examples = model.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print example\n",
    "print(model.wv.vectors.shape)   # model.wv.vectors == model.wv.syn0\n",
    "# print(model.wv['organic'])\n",
    "# print(model.wv.index2word[100], model.wv.vectors[100])\n",
    "# before_example = model.wv['why']\n",
    "# print(before_example)\n",
    "\n",
    "# (59861, 300) for min_count=1\n",
    "# (19160, 300) for min_count=5\n",
    "# (16675, 300) for min_count=5 after stop_words and lemmatize\n",
    "# (16803, 300) for min_count=5 after lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(model.wv.vocab['organic'], model.wv.vocab['organic'].index)\n",
    "print(model.wv.index2word[15])\n",
    "print((model.wv.word_vec('organic') == model.wv.vectors[15]).all())\n",
    "print((model.wv.get_vector('organic') == model.wv.word_vec('organic')).all())\n",
    "print(list(model.wv.vocab)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA should be run on the whole corpus?\n",
    "def plot_pca_samples(model, word_list, fname=None):\n",
    "    '''\n",
    "    model: Word2Vec\n",
    "    word_list: a list of words\n",
    "    '''\n",
    "    plt.figure(figsize=(15,15))\n",
    "    idx = [model.wv.vocab[word].index for word in word_list]\n",
    "    # X = model.wv[word_list]\n",
    "    X = model.wv.vectors\n",
    "    pca = PCA(n_components=2)\n",
    "    result = pca.fit_transform(X)\n",
    "    result = result[idx]\n",
    "    plt.scatter(result[:, 0], result[0:, 1])\n",
    "    for i, word in enumerate(word_list):\n",
    "        plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "    if fname:\n",
    "        plt.savefig(fname)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot after initialization\n",
    "draw_vocab = list(model.wv.vocab)[50:100]\n",
    "print(draw_vocab)\n",
    "plot_pca_samples(model, draw_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# model_pretrained = KeyedVectors.load_word2vec_format(\"../wv/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "# model.build_vocab([list(model_pretrained.vocab.keys())], update=True)     # why size of vocab remains the same?\n",
    "model.intersect_word2vec_format('../wv/GoogleNews-vectors-negative300.bin.gz', binary=True, lockf=1.0)\n",
    "# pre-trained word2vec models can be downloaded from https://code.google.com/archive/p/word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the difference between pre-trained model and initialization\n",
    "plot_pca_samples(model, draw_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# TODO: use sentences_tokenized or sentences_step2 ???\n",
    "# model.train(sentences_tokenized, total_examples=total_examples, epochs=5)\n",
    "model.train(sentences_step2, total_examples=total_examples, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the difference between pre-trained and fine-tuning \n",
    "plot_pca_samples(model, draw_vocab, 'train_raw.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model, change the file names to indicate specific models\n",
    "'''\n",
    "_comment: text from comment\n",
    "_raw: not trained on the corpus, pre-trained model\n",
    "_trained: trained on the corpus\n",
    "_stopword: without stopword\n",
    "'''\n",
    "# model.save(\"../wv/w2v_comment_raw.model\")\n",
    "model.save(\"../wv/w2v_comment_trained.model\")\n",
    "# model.save(\"../wv/w2v_comment_stopword_raw.model\")\n",
    "# model.save(\"../wv/w2v_comment_stopword_trained.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
